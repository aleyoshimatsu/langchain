{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10413396,"sourceType":"datasetVersion","datasetId":6453746},{"sourceId":10419063,"sourceType":"datasetVersion","datasetId":6457514}],"dockerImageVersionId":30822,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -qU docling langchain","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:14:21.743294Z","iopub.execute_input":"2025-01-10T08:14:21.743594Z","iopub.status.idle":"2025-01-10T08:14:35.073289Z","shell.execute_reply.started":"2025-01-10T08:14:21.743555Z","shell.execute_reply":"2025-01-10T08:14:35.072482Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loading the docling pdf file","metadata":{}},{"cell_type":"code","source":"from docling.document_converter import DocumentConverter\n\nDOC_SOURCE = \"/kaggle/input/india-docling-dataset/Hongkong - April 2024 7 PAX Mr. Suraj.pdf\"\n\ndoc = DocumentConverter().convert(source=DOC_SOURCE).document","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T07:28:30.093061Z","iopub.execute_input":"2025-01-10T07:28:30.093435Z","iopub.status.idle":"2025-01-10T07:31:43.671654Z","shell.execute_reply.started":"2025-01-10T07:28:30.093407Z","shell.execute_reply":"2025-01-10T07:31:43.670831Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Chunking of pdf file with HybridChunker with chunker.serialize(chunk=chunk) fuction","metadata":{}},{"cell_type":"code","source":"from docling.chunking import HybridChunker\n\nchunker = HybridChunker()\nchunk_iter = chunker.chunk(doc)\nprint(list(chunk_iter)[11])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:03:34.613009Z","iopub.execute_input":"2025-01-10T08:03:34.613402Z","iopub.status.idle":"2025-01-10T08:03:34.769144Z","shell.execute_reply.started":"2025-01-10T08:03:34.613372Z","shell.execute_reply":"2025-01-10T08:03:34.768153Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i, chunk in enumerate(chunk_iter):\n    print(f\"=== {i} ===\")\n    print(f\"chunk.text:\\n{repr(f'{chunk.text[:300]}…')}\")\n\n    enriched_text = chunker.serialize(chunk=chunk)\n    print(f\"chunker.serialize(chunk):\\n{repr(f'{enriched_text[:300]}…')}\")\n    print(f\"###Serialized Text (Full): {enriched_text}\")\n    print(f\"Chunk Text: {chunk.text}\")\n    print(f\"Chunk Meta: {chunk.meta}\")\n    print(\"-\" * 20)\n    # Inspect the chunk's attributes\n    print(f\"###Chunk attributes: {vars(chunk)}\")\n\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T07:34:14.813013Z","iopub.execute_input":"2025-01-10T07:34:14.813423Z","iopub.status.idle":"2025-01-10T07:34:14.839301Z","shell.execute_reply.started":"2025-01-10T07:34:14.813378Z","shell.execute_reply":"2025-01-10T07:34:14.838454Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Map Chunks to LangChain Documents:\n","metadata":{}},{"cell_type":"code","source":"import json\nfrom docling.document_converter import DocumentConverter\nfrom docling.chunking import HybridChunker\nfrom langchain.docstore.document import Document\nimport json\n\nDOC_SOURCE = \"/kaggle/input/india-docling-dataset/Hongkong - April 2024 7 PAX Mr. Suraj.pdf\"\n\n# 1. Convert the document\ndoc = DocumentConverter().convert(source=DOC_SOURCE).document\n\n# 2. Initialize the chunker\nchunker = HybridChunker()\nchunk_iter = chunker.chunk(doc)\n\ndef chunk_to_langchain_document(chunk_text, chunk_meta):\n    \"\"\"\n    Transforms a chunk of text and its metadata into a LangChain Document.\n\n    Args:\n        chunk_text (str): The text content of the chunk.\n        chunk_meta (dict): The metadata associated with the chunk.\n\n    Returns:\n        langchain.docstore.document.Document: A LangChain Document object.\n    \"\"\"\n\n    metadata = {\n        \"schema_name\": chunk_meta.get(\"schema_name\"),\n        \"version\": chunk_meta.get(\"version\"),\n        \"headings\": chunk_meta.get(\"headings\"),\n        \"captions\": chunk_meta.get(\"captions\"),\n    }\n\n    # Extract relevant info from doc_items\n    if chunk_meta.get(\"doc_items\"):\n        doc_item = chunk_meta[\"doc_items\"][0]\n        metadata[\"doc_item_self_ref\"] = doc_item.get(\"self_ref\")\n        metadata[\"doc_item_parent\"] = doc_item.get(\"parent\")\n        metadata[\"doc_item_label\"] = doc_item.get(\"label\")\n\n        if doc_item.get(\"prov\"):\n            prov = doc_item[\"prov\"][0]\n            metadata[\"page_no\"] = prov.get(\"page_no\")\n            metadata[\"bbox\"] = prov.get(\"bbox\")\n            metadata[\"charspan\"] = prov.get(\"charspan\")\n\n    # Extract relevant info from origin\n    if chunk_meta.get(\"origin\"):\n        origin = chunk_meta[\"origin\"]\n        metadata[\"mimetype\"] = origin.get(\"mimetype\")\n        metadata[\"binary_hash\"] = origin.get(\"binary_hash\")\n        metadata[\"filename\"] = origin.get(\"filename\")\n        metadata[\"uri\"] = origin.get(\"uri\")\n\n    return Document(page_content=chunk_text, metadata=metadata)\n\n# 3. Process each chunk\nlangchain_documents = []\nfor i, chunk in enumerate(chunk_iter):\n    print(f\"=== {i} ===\")\n    print(f\"chunk.text:\\n{repr(f'{chunk.text[:300]}…')}\")\n\n    # 4. Inspect the chunk object\n    print(f\"###Chunk attributes before serialization: {vars(chunk)}\")\n\n    # 5. Serialize the chunk\n    enriched_text = chunker.serialize(chunk=chunk)\n    print(f\"chunker.serialize(chunk):\\n{repr(f'{enriched_text[:300]}…')}\")\n    print(f\"###Serialized Text (Full): {enriched_text}\")\n    print(f\"Chunk Text: {chunk.text}\")\n    print(f\"Chunk Meta: {chunk.meta}\")\n    print(\"-\" * 20)\n\n    # 6. Check if enriched_text is empty\n    if not enriched_text:\n        print(f\"Error: enriched_text is empty for chunk {i}. Skipping this chunk.\")\n        continue  # Skip to the next chunk\n\n    # 7. Clean up the string\n    try:\n        cleaned_text = enriched_text.strip()  # Remove leading/trailing whitespace\n        deserialized_data = json.loads(cleaned_text)\n        chunk_text = deserialized_data[\"chunk_text\"]\n        chunk_meta = deserialized_data[\"chunk_meta\"]\n\n        # 8. Create the LangChain Document\n        langchain_doc = chunk_to_langchain_document(chunk_text, chunk_meta)\n        langchain_documents.append(langchain_doc)\n    except json.JSONDecodeError as e:\n        print(f\"Error: JSONDecodeError for chunk {i}: {e}. Skipping this chunk.\")\n        continue # Skip to the next chunk\n\n# 9. Print the LangChain Documents\nprint(\"\\n--- LangChain Documents ---\")\nfor i, doc in enumerate(langchain_documents):\n    print(f\"LangChain Document {i+1}:\")\n    print(f\"  Page Content: {doc.page_content}\")\n    print(f\"  Metadata: {json.dumps(doc.metadata, indent=2)}\")\n    print(\"-\" * 20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:00:31.618198Z","iopub.execute_input":"2025-01-10T08:00:31.61867Z","iopub.status.idle":"2025-01-10T08:03:12.256888Z","shell.execute_reply.started":"2025-01-10T08:00:31.618639Z","shell.execute_reply":"2025-01-10T08:03:12.255795Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from docling.document_converter import DocumentConverter\n\nsource = \"/kaggle/input/india-docling-dataset/Ladakh - June 2024 Ms Anjali.docx\"  # PDF path or URL\nconverter = DocumentConverter()\nresult = converter.convert(source)\nprint(result.document.export_to_markdown()) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:57:39.813359Z","iopub.execute_input":"2025-01-10T08:57:39.813697Z","iopub.status.idle":"2025-01-10T08:57:40.50504Z","shell.execute_reply.started":"2025-01-10T08:57:39.813669Z","shell.execute_reply":"2025-01-10T08:57:40.504276Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Custom conversion\nCore Idea: Exploring Different Conversion Configurations\n\nThe code you've provided is designed to showcase the flexibility of the docling library by allowing you to experiment with various combinations of:\n\nPDF Backends: Different libraries used to parse and extract data from PDF files.\n\nOCR Engines: Different libraries used to perform Optical Character Recognition.\n\nPipeline Options: Settings that control how the document is processed, including OCR, table structure detection, and hardware acceleration.\n\n\nHow It Works\n\nThe code works by:\n\nLoading a PDF document.\n\nConfiguring the document conversion process by choosing a PDF backend, OCR engine, and pipeline options.\n\nConverting the PDF document using the specified configuration.\n\nMeasuring the time taken for conversion.\n\nExporting the converted document to various formats (JSON, text, Markdown, document tags).\n\n","metadata":{}},{"cell_type":"code","source":"import json\nimport logging\nimport time\nfrom pathlib import Path\n\nfrom docling.backend.pypdfium2_backend import PyPdfiumDocumentBackend\nfrom docling.datamodel.base_models import InputFormat\nfrom docling.datamodel.pipeline_options import PdfPipelineOptions\nfrom docling.document_converter import DocumentConverter, PdfFormatOption\nfrom docling.models.ocr_mac_model import OcrMacOptions\nfrom docling.models.tesseract_ocr_cli_model import TesseractCliOcrOptions\nfrom docling.models.tesseract_ocr_model import TesseractOcrOptions\nfrom docling.backend.docling_parse_backend import DoclingParseDocumentBackend\nfrom docling.datamodel.base_models import InputFormat\nfrom docling.datamodel.pipeline_options import (\n    AcceleratorDevice,\n    AcceleratorOptions,\n    PdfPipelineOptions,\n    TesseractCliOcrOptions,\n    TesseractOcrOptions,\n)\nfrom docling.datamodel.settings import settings\nfrom docling.document_converter import DocumentConverter, PdfFormatOption\n\n_log = logging.getLogger(__name__)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def main():\n    logging.basicConfig(level=logging.INFO)\n\n    input_doc_path = Path(\"/kaggle/input/india-docling-dataset/Himachal Feb-Mar 2024.pdf\")\n\n    ###########################################################################\n\n    # The following sections contain a combination of PipelineOptions\n    # and PDF Backends for various configurations.\n    # Uncomment one section at the time to see the differences in the output.\n\n    # PyPdfium without EasyOCR\n    # --------------------\n    # pipeline_options = PdfPipelineOptions()\n    # pipeline_options.do_ocr = False\n    # pipeline_options.do_table_structure = True\n    # pipeline_options.table_structure_options.do_cell_matching = False\n\n    # doc_converter = DocumentConverter(\n    #     format_options={\n    #         InputFormat.PDF: PdfFormatOption(\n    #             pipeline_options=pipeline_options, backend=PyPdfiumDocumentBackend\n    #         )\n    #     }\n    # )\n\n    # PyPdfium with EasyOCR\n    # -----------------\n    # pipeline_options = PdfPipelineOptions()\n    # pipeline_options.do_ocr = True\n    # pipeline_options.do_table_structure = True\n    # pipeline_options.table_structure_options.do_cell_matching = True\n\n    # doc_converter = DocumentConverter(\n    #     format_options={\n    #         InputFormat.PDF: PdfFormatOption(\n    #             pipeline_options=pipeline_options, backend=PyPdfiumDocumentBackend\n    #         )\n    #     }\n    # )\n\n    # Docling Parse without EasyOCR\n    # -------------------------\n    # pipeline_options = PdfPipelineOptions()\n    # pipeline_options.do_ocr = False\n    # pipeline_options.do_table_structure = True\n    # pipeline_options.table_structure_options.do_cell_matching = True\n\n    # doc_converter = DocumentConverter(\n    #     format_options={\n    #         InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n    #     }\n    # )\n\n    # Docling Parse with EasyOCR\n    # ----------------------\n    pipeline_options = PdfPipelineOptions()\n    pipeline_options.do_ocr = True\n    pipeline_options.do_table_structure = True\n    pipeline_options.table_structure_options.do_cell_matching = True\n    pipeline_options.ocr_options.lang = [\"es\"]\n    pipeline_options.accelerator_options = AcceleratorOptions(\n        num_threads=4, device='auto',\n    )\n\n    doc_converter = DocumentConverter(\n        format_options={\n            InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n        }\n    )\n\n    # Docling Parse with EasyOCR (CPU only)\n    # ----------------------\n    # pipeline_options = PdfPipelineOptions()\n    # pipeline_options.do_ocr = True\n    # pipeline_options.ocr_options.use_gpu = False  # <-- set this.\n    # pipeline_options.do_table_structure = True\n    # pipeline_options.table_structure_options.do_cell_matching = True\n\n    # doc_converter = DocumentConverter(\n    #     format_options={\n    #         InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n    #     }\n    # )\n\n    # Docling Parse with Tesseract\n    # ----------------------\n    # pipeline_options = PdfPipelineOptions()\n    # pipeline_options.do_ocr = True\n    # pipeline_options.do_table_structure = True\n    # pipeline_options.table_structure_options.do_cell_matching = True\n    # pipeline_options.ocr_options = TesseractOcrOptions()\n\n    # doc_converter = DocumentConverter(\n    #     format_options={\n    #         InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n    #     }\n    # )\n\n    # Docling Parse with Tesseract CLI\n    # ----------------------\n    # pipeline_options = PdfPipelineOptions()\n    # pipeline_options.do_ocr = True\n    # pipeline_options.do_table_structure = True\n    # pipeline_options.table_structure_options.do_cell_matching = True\n    # pipeline_options.ocr_options = TesseractCliOcrOptions()\n\n    # doc_converter = DocumentConverter(\n    #     format_options={\n    #         InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n    #     }\n    # )\n\n    # Docling Parse with ocrmac(Mac only)\n    # ----------------------\n    # pipeline_options = PdfPipelineOptions()\n    # pipeline_options.do_ocr = True\n    # pipeline_options.do_table_structure = True\n    # pipeline_options.table_structure_options.do_cell_matching = True\n    # pipeline_options.ocr_options = OcrMacOptions()\n\n    # doc_converter = DocumentConverter(\n    #     format_options={\n    #         InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n    #     }\n    # )\n\n    ###########################################################################\n\n    start_time = time.time()\n    conv_result = doc_converter.convert(input_doc_path)\n    end_time = time.time() - start_time\n\n    _log.info(f\"Document converted in {end_time:.2f} seconds.\")\n\n    ## Export results\n    output_dir = Path(\"scratch_Custom_conversion\")\n    output_dir.mkdir(parents=True, exist_ok=True)\n    doc_filename = conv_result.input.file.stem\n\n    # Export Deep Search document JSON format:\n    with (output_dir / f\"{doc_filename}.json\").open(\"w\", encoding=\"utf-8\") as fp:\n        fp.write(json.dumps(conv_result.document.export_to_dict()))\n\n    # Export Text format:\n    with (output_dir / f\"{doc_filename}.txt\").open(\"w\", encoding=\"utf-8\") as fp:\n        fp.write(conv_result.document.export_to_text())\n\n    # Export Markdown format:\n    with (output_dir / f\"{doc_filename}.md\").open(\"w\", encoding=\"utf-8\") as fp:\n        fp.write(conv_result.document.export_to_markdown())\n\n    # Export Document Tags format:\n    with (output_dir / f\"{doc_filename}.doctags\").open(\"w\", encoding=\"utf-8\") as fp:\n        fp.write(conv_result.document.export_to_document_tokens())\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T09:51:09.904146Z","iopub.execute_input":"2025-01-10T09:51:09.90451Z","iopub.status.idle":"2025-01-10T09:51:15.875777Z","shell.execute_reply.started":"2025-01-10T09:51:09.904484Z","shell.execute_reply":"2025-01-10T09:51:15.875098Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Batch conversion\n\n\nCore Idea: Processing Multiple Documents in a Batch\n\nThe code you've provided demonstrates how to use the docling library to convert multiple documents in a single run (a batch) and then export the results of each conversion to various formats. This is useful for processing large collections of documents efficiently.\n\n\n\nIn Summary\n\nThe batch conversion method you've provided is currently limited to PDF files due to its reliance on PDF-specific classes, configurations, and internal logic. To process other file types, you would need to create new pipelines and configurations tailored to those specific formats.\n\nIf you need to process other file types, you'll need to explore the docling library's documentation to see if it provides support for those formats or if you need to build custom pipelines.\n\nLet me know if you have any more questions about this!","metadata":{}},{"cell_type":"code","source":"import json\nimport logging\nimport time\nfrom pathlib import Path\nfrom typing import Iterable\n\nimport yaml\n\nfrom docling.datamodel.base_models import ConversionStatus\nfrom docling.datamodel.document import ConversionResult\nfrom docling.datamodel.settings import settings\nfrom docling.document_converter import DocumentConverter\n\n_log = logging.getLogger(__name__)\n\nUSE_V2 = True\nUSE_LEGACY = True\n\ndef export_documents(\n    conv_results: Iterable[ConversionResult],\n    output_dir: Path,\n):\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    success_count = 0\n    failure_count = 0\n    partial_success_count = 0\n\n    for conv_res in conv_results:\n        if conv_res.status == ConversionStatus.SUCCESS:\n            success_count += 1\n            doc_filename = conv_res.input.file.stem\n\n            if USE_V2:\n                # Export Docling document format to JSON:\n                with (output_dir / f\"{doc_filename}.json\").open(\"w\", encoding=\"utf-8\") as fp:\n                    fp.write(json.dumps(conv_res.document.export_to_dict()))\n\n                # Export Docling document format to YAML:\n                with (output_dir / f\"{doc_filename}.yaml\").open(\"w\", encoding=\"utf-8\") as fp:\n                    fp.write(yaml.safe_dump(conv_res.document.export_to_dict()))\n\n                # Export Docling document format to doctags:\n                with (output_dir / f\"{doc_filename}.doctags.txt\").open(\"w\", encoding=\"utf-8\") as fp:\n                    fp.write(conv_res.document.export_to_document_tokens())\n\n                # Export Docling document format to markdown:\n                with (output_dir / f\"{doc_filename}.md\").open(\"w\", encoding=\"utf-8\") as fp:\n                    fp.write(conv_res.document.export_to_markdown())\n\n                # Export Docling document format to text:\n                with (output_dir / f\"{doc_filename}.txt\").open(\"w\", encoding=\"utf-8\") as fp:\n                    fp.write(conv_res.document.export_to_markdown(strict_text=True))\n\n            if USE_LEGACY:\n                # Export Deep Search document JSON format:\n                with (output_dir / f\"{doc_filename}.legacy.json\").open(\n                    \"w\", encoding=\"utf-8\"\n                ) as fp:\n                    fp.write(json.dumps(conv_res.legacy_document.export_to_dict()))\n\n                # Export Text format:\n                with (output_dir / f\"{doc_filename}.legacy.txt\").open(\n                    \"w\", encoding=\"utf-8\"\n                ) as fp:\n                    fp.write(\n                        conv_res.legacy_document.export_to_markdown(strict_text=True)\n                    )\n\n                # Export Markdown format:\n                with (output_dir / f\"{doc_filename}.legacy.md\").open(\n                    \"w\", encoding=\"utf-8\"\n                ) as fp:\n                    fp.write(conv_res.legacy_document.export_to_markdown())\n\n                # Export Document Tags format:\n                with (output_dir / f\"{doc_filename}.legacy.doctags.txt\").open(\n                    \"w\", encoding=\"utf-8\"\n                ) as fp:\n                    fp.write(conv_res.legacy_document.export_to_document_tokens())\n\n        elif conv_res.status == ConversionStatus.PARTIAL_SUCCESS:\n            _log.info(\n                f\"Document {conv_res.input.file} was partially converted with the following errors:\"\n            )\n            for item in conv_res.errors:\n                _log.info(f\"\\t{item.error_message}\")\n            partial_success_count += 1\n        else:\n            _log.info(f\"Document {conv_res.input.file} failed to convert.\")\n            failure_count += 1\n\n    _log.info(\n        f\"Processed {success_count + partial_success_count + failure_count} docs, \"\n        f\"of which {failure_count} failed \"\n        f\"and {partial_success_count} were partially converted.\"\n    )\n    return success_count, partial_success_count, failure_count\n\n\ndef main():\n    logging.basicConfig(level=logging.INFO)\n\n    input_doc_paths = [\n        Path(\"/kaggle/input/india-docling-dataset/Himachal Feb-Mar 2024.pdf\"),\n        Path(\"/kaggle/input/india-docling-dataset/Ladakh.pdf\"),\n        Path(\"/kaggle/input/india-docling-dataset/Ladakh - June 2024 Ms Anjali.pdf\"),\n        Path(\"/kaggle/input/india-docling-dataset/Hongkong - April 2024 7 PAX Mr. Suraj.pdf\"),\n    ]\n\n    # buf = BytesIO(Path(\"./test/data/2206.01062.pdf\").open(\"rb\").read())\n    # docs = [DocumentStream(name=\"my_doc.pdf\", stream=buf)]\n    # input = DocumentConversionInput.from_streams(docs)\n\n    # # Turn on inline debug visualizations:\n    # settings.debug.visualize_layout = True\n    # settings.debug.visualize_ocr = True\n    # settings.debug.visualize_tables = True\n    # settings.debug.visualize_cells = True\n\n    doc_converter = DocumentConverter()\n\n    start_time = time.time()\n\n    conv_results = doc_converter.convert_all(\n        input_doc_paths,\n        raises_on_error=False,  # to let conversion run through all and examine results at the end\n    )\n    success_count, partial_success_count, failure_count = export_documents(\n        conv_results, output_dir=Path(\"scratch_Batch_conversion\")\n    )\n\n    end_time = time.time() - start_time\n\n    _log.info(f\"Document conversion complete in {end_time:.2f} seconds.\")\n\n    if failure_count > 0:\n        raise RuntimeError(\n            f\"The example failed converting {failure_count} on {len(input_doc_paths)}.\"\n        )\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T09:58:12.661248Z","iopub.execute_input":"2025-01-10T09:58:12.661604Z","iopub.status.idle":"2025-01-10T09:58:26.781316Z","shell.execute_reply.started":"2025-01-10T09:58:12.661578Z","shell.execute_reply":"2025-01-10T09:58:26.780361Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Multi-format conversion","metadata":{}},{"cell_type":"code","source":"import json\nimport logging\nfrom pathlib import Path\n\nimport yaml\n\nfrom docling.backend.pypdfium2_backend import PyPdfiumDocumentBackend\nfrom docling.datamodel.base_models import InputFormat\nfrom docling.document_converter import (\n    DocumentConverter,\n    PdfFormatOption,\n    WordFormatOption,\n)\nfrom docling.pipeline.simple_pipeline import SimplePipeline\nfrom docling.pipeline.standard_pdf_pipeline import StandardPdfPipeline\n\n_log = logging.getLogger(__name__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T09:46:10.541798Z","iopub.execute_input":"2025-01-10T09:46:10.542159Z","iopub.status.idle":"2025-01-10T09:46:10.547893Z","shell.execute_reply.started":"2025-01-10T09:46:10.54213Z","shell.execute_reply":"2025-01-10T09:46:10.547212Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport logging\nfrom pathlib import Path\nimport yaml\nfrom docling.backend.pypdfium2_backend import PyPdfiumDocumentBackend\nfrom docling.datamodel.base_models import InputFormat\nfrom docling.document_converter import (\n    DocumentConverter,\n    PdfFormatOption,\n    WordFormatOption,\n)\nfrom docling.pipeline.simple_pipeline import SimplePipeline\nfrom docling.pipeline.standard_pdf_pipeline import StandardPdfPipeline\n\n_log = logging.getLogger(__name__)\n\ndef main():\n    input_paths = [\n        Path(\"/kaggle/input/india-docling-dataset/Himachal Feb-Mar 2024.pdf\"),\n        Path(\"/kaggle/input/india-docling-dataset/Himachal Feb-Mar 2024.pptx\"),\n        Path(\"/kaggle/input/india-docling-dataset/Hongkong - April 2024 7 PAX Mr. Suraj.pdf\"),\n        Path(\"/kaggle/input/india-docling-dataset/Ladakh - June 2024 Ms Anjali.docx\"),\n        Path(\"/kaggle/input/india-docling-dataset/Ladakh - June 2024 Ms Anjali.pdf\"),\n        Path(\"/kaggle/input/india-docling-dataset/Ladakh.docx\"),\n        Path(\"/kaggle/input/india-docling-dataset/Ladakh.pdf\"),\n        #Path(\"tests/data/test_01.asciidoc\"),\n        #Path(\"tests/data/test_01.asciidoc\"),\n    ]\n\n\n    doc_converter = (\n        DocumentConverter(\n            allowed_formats=[\n                InputFormat.PDF,\n                InputFormat.IMAGE,\n                InputFormat.DOCX,\n                InputFormat.HTML,\n                InputFormat.PPTX,\n                InputFormat.ASCIIDOC,\n                InputFormat.MD,\n            ],\n            format_options={\n                InputFormat.PDF: PdfFormatOption(\n                    pipeline_cls=StandardPdfPipeline, backend=PyPdfiumDocumentBackend\n                ),\n                InputFormat.DOCX: WordFormatOption(\n                    pipeline_cls=SimplePipeline\n                ),\n            },\n        )\n    )\n\n    conv_results = doc_converter.convert_all(input_paths)\n\n    for res in conv_results:\n        out_path = Path(\"scratch_Multi-format_conversion\")  # Define the output directory\n        out_path.mkdir(parents=True, exist_ok=True)  # Create the directory if it doesn't exist\n        print(\n            f\"Document {res.input.file.name} converted.\"\n            f\"\\nSaved markdown output to: {str(out_path)}\"\n        )\n        _log.debug(res.document._export_to_indented_text(max_text_len=16))\n        # Export Docling document format to markdowndoc:\n        with (out_path / f\"{res.input.file.stem}.md\").open(\"w\", encoding=\"utf-8\") as fp:\n            fp.write(res.document.export_to_markdown())\n\n        with (out_path / f\"{res.input.file.stem}.json\").open(\"w\", encoding=\"utf-8\") as fp:\n            fp.write(json.dumps(res.document.export_to_dict()))\n\n        with (out_path / f\"{res.input.file.stem}.yaml\").open(\"w\", encoding=\"utf-8\") as fp:\n            fp.write(yaml.safe_dump(res.document.export_to_dict()))\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T09:46:50.778046Z","iopub.execute_input":"2025-01-10T09:46:50.778568Z","iopub.status.idle":"2025-01-10T09:47:12.615352Z","shell.execute_reply.started":"2025-01-10T09:46:50.778541Z","shell.execute_reply":"2025-01-10T09:47:12.614536Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Figure Import","metadata":{}},{"cell_type":"code","source":"for res in conv_results:\n        out_path = Path(\"scratch_1\")  # Define the output directory\n        out_path.mkdir(parents=True, exist_ok=True)  # Create the directory if it doesn't exist\n        print(\n            f\"Document {res.input.file.name} converted.\"\n            f\"\\nSaved markdown output to: {str(out_path)}\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Output of a .pdf file using Figure Import","metadata":{}},{"cell_type":"code","source":"import logging\nimport time\nfrom pathlib import Path\nfrom docling_core.types.doc import ImageRefMode, PictureItem, TableItem\nfrom docling.datamodel.base_models import FigureElement, InputFormat, Table\nfrom docling.datamodel.pipeline_options import PdfPipelineOptions\nfrom docling.document_converter import DocumentConverter, PdfFormatOption\n_log = logging.getLogger(__name__)\n_log = logging.getLogger(__name__)\nIMAGE_RESOLUTION_SCALE = 2.0\ndef main():\n    logging.basicConfig(level=logging.INFO)\n\n    input_doc_path = Path(\"/kaggle/input/india-docling-dataset/Himachal Feb-Mar 2024.pdf\")\n    #output_dir = Path(\"scratch\")\n    output_dir = Path(\"scratch_Figure_Import\")  # Define the output directory\n    output_dir.mkdir(parents=True, exist_ok=True)  # Create the directory if it doesn't exist\n\n    # Important: For operating with page images, we must keep them, otherwise the DocumentConverter\n    # will destroy them for cleaning up memory.\n    # This is done by setting PdfPipelineOptions.images_scale, which also defines the scale of images.\n    # scale=1 correspond of a standard 72 DPI image\n    # The PdfPipelineOptions.generate_* are the selectors for the document elements which will be enriched\n    # with the image field\n    pipeline_options = PdfPipelineOptions()\n    pipeline_options.images_scale = IMAGE_RESOLUTION_SCALE\n    pipeline_options.generate_page_images = True\n    pipeline_options.generate_picture_images = True\n\n    doc_converter = DocumentConverter(\n        format_options={\n            InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n        }\n    )\n\n    start_time = time.time()\n\n    conv_res = doc_converter.convert(input_doc_path)\n\n    output_dir.mkdir(parents=True, exist_ok=True)\n    doc_filename = conv_res.input.file.stem\n\n    # Save page images\n    for page_no, page in conv_res.document.pages.items():\n        page_no = page.page_no\n        page_image_filename = output_dir / f\"{doc_filename}-{page_no}.png\"\n        with page_image_filename.open(\"wb\") as fp:\n            page.image.pil_image.save(fp, format=\"PNG\")\n\n    # Save images of figures and tables\n    table_counter = 0\n    picture_counter = 0\n    for element, _level in conv_res.document.iterate_items():\n        if isinstance(element, TableItem):\n            table_counter += 1\n            element_image_filename = (\n                output_dir / f\"{doc_filename}-table-{table_counter}.png\"\n            )\n            with element_image_filename.open(\"wb\") as fp:\n                element.get_image(conv_res.document).save(fp, \"PNG\")\n\n        if isinstance(element, PictureItem):\n            picture_counter += 1\n            element_image_filename = (\n                output_dir / f\"{doc_filename}-picture-{picture_counter}.png\"\n            )\n            with element_image_filename.open(\"wb\") as fp:\n                element.get_image(conv_res.document).save(fp, \"PNG\")\n\n    # Save markdown with embedded pictures\n    md_filename = output_dir / f\"{doc_filename}-with-images.md\"\n    conv_res.document.save_as_markdown(md_filename, image_mode=ImageRefMode.EMBEDDED)\n\n    # Save markdown with externally referenced pictures\n    md_filename = output_dir / f\"{doc_filename}-with-image-refs.md\"\n    conv_res.document.save_as_markdown(md_filename, image_mode=ImageRefMode.REFERENCED)\n\n    # Save HTML with externally referenced pictures\n    html_filename = output_dir / f\"{doc_filename}-with-image-refs.html\"\n    conv_res.document.save_as_html(html_filename, image_mode=ImageRefMode.REFERENCED)\n\n    end_time = time.time() - start_time\n\n    _log.info(f\"Document converted and figures exported in {end_time:.2f} seconds.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T09:02:18.657966Z","iopub.execute_input":"2025-01-10T09:02:18.658356Z","iopub.status.idle":"2025-01-10T09:02:18.667285Z","shell.execute_reply.started":"2025-01-10T09:02:18.658326Z","shell.execute_reply":"2025-01-10T09:02:18.666401Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T09:02:21.118301Z","iopub.execute_input":"2025-01-10T09:02:21.118586Z","iopub.status.idle":"2025-01-10T09:02:31.007385Z","shell.execute_reply.started":"2025-01-10T09:02:21.118567Z","shell.execute_reply":"2025-01-10T09:02:31.006475Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Output of a .docx file using Figure Import","metadata":{}},{"cell_type":"code","source":"import logging\nimport time\nfrom pathlib import Path\nfrom docling_core.types.doc import ImageRefMode, PictureItem, TableItem\nfrom docling.datamodel.base_models import FigureElement, InputFormat, Table\nfrom docling.datamodel.pipeline_options import PdfPipelineOptions\nfrom docling.document_converter import DocumentConverter, PdfFormatOption\n_log = logging.getLogger(__name__)\n_log = logging.getLogger(__name__)\nIMAGE_RESOLUTION_SCALE = 2.0\ndef main():\n    logging.basicConfig(level=logging.INFO)\n\n    input_doc_path = Path(\"/kaggle/input/india-docling-dataset/Ladakh.docx\")\n    #output_dir = Path(\"scratch\")\n    output_dir = Path(\"scratch_Figure_Import\")  # Define the output directory\n    output_dir.mkdir(parents=True, exist_ok=True)  # Create the directory if it doesn't exist\n\n    # Important: For operating with page images, we must keep them, otherwise the DocumentConverter\n    # will destroy them for cleaning up memory.\n    # This is done by setting PdfPipelineOptions.images_scale, which also defines the scale of images.\n    # scale=1 correspond of a standard 72 DPI image\n    # The PdfPipelineOptions.generate_* are the selectors for the document elements which will be enriched\n    # with the image field\n    pipeline_options = PdfPipelineOptions()\n    pipeline_options.images_scale = IMAGE_RESOLUTION_SCALE\n    pipeline_options.generate_page_images = True\n    pipeline_options.generate_picture_images = True\n\n    doc_converter = DocumentConverter(\n        format_options={\n            InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n        }\n    )\n\n    start_time = time.time()\n\n    conv_res = doc_converter.convert(input_doc_path)\n\n    output_dir.mkdir(parents=True, exist_ok=True)\n    doc_filename = conv_res.input.file.stem\n\n    # Save page images\n    for page_no, page in conv_res.document.pages.items():\n        page_no = page.page_no\n        page_image_filename = output_dir / f\"{doc_filename}-{page_no}.png\"\n        with page_image_filename.open(\"wb\") as fp:\n            page.image.pil_image.save(fp, format=\"PNG\")\n\n    # Save images of figures and tables\n    table_counter = 0\n    picture_counter = 0\n    for element, _level in conv_res.document.iterate_items():\n        if isinstance(element, TableItem):\n            table_counter += 1\n            element_image_filename = (\n                output_dir / f\"{doc_filename}-table-{table_counter}.png\"\n            )\n            with element_image_filename.open(\"wb\") as fp:\n                element.get_image(conv_res.document).save(fp, \"PNG\")\n\n        if isinstance(element, PictureItem):\n            picture_counter += 1\n            element_image_filename = (\n                output_dir / f\"{doc_filename}-picture-{picture_counter}.png\"\n            )\n            with element_image_filename.open(\"wb\") as fp:\n                element.get_image(conv_res.document).save(fp, \"PNG\")\n\n    # Save markdown with embedded pictures\n    md_filename = output_dir / f\"{doc_filename}-with-images.md\"\n    conv_res.document.save_as_markdown(md_filename, image_mode=ImageRefMode.EMBEDDED)\n\n    # Save markdown with externally referenced pictures\n    md_filename = output_dir / f\"{doc_filename}-with-image-refs.md\"\n    conv_res.document.save_as_markdown(md_filename, image_mode=ImageRefMode.REFERENCED)\n\n    # Save HTML with externally referenced pictures\n    html_filename = output_dir / f\"{doc_filename}-with-image-refs.html\"\n    conv_res.document.save_as_html(html_filename, image_mode=ImageRefMode.REFERENCED)\n\n    end_time = time.time() - start_time\n\n    _log.info(f\"Document converted and figures exported in {end_time:.2f} seconds.\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T09:03:41.737649Z","iopub.execute_input":"2025-01-10T09:03:41.737938Z","iopub.status.idle":"2025-01-10T09:03:44.058498Z","shell.execute_reply.started":"2025-01-10T09:03:41.737916Z","shell.execute_reply":"2025-01-10T09:03:44.057732Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Output of a .pptx file using Figure Import","metadata":{}},{"cell_type":"code","source":"import logging\nimport time\nfrom pathlib import Path\nfrom docling_core.types.doc import ImageRefMode, PictureItem, TableItem\nfrom docling.datamodel.base_models import FigureElement, InputFormat, Table\nfrom docling.datamodel.pipeline_options import PdfPipelineOptions\nfrom docling.document_converter import DocumentConverter, PdfFormatOption\n_log = logging.getLogger(__name__)\n_log = logging.getLogger(__name__)\nIMAGE_RESOLUTION_SCALE = 2.0\ndef main():\n    logging.basicConfig(level=logging.INFO)\n\n    input_doc_path = Path(\"/kaggle/input/india-docling-dataset/Himachal Feb-Mar 2024.pptx\")\n    #output_dir = Path(\"scratch\")\n    output_dir = Path(\"scratch_Figure_Import\")  # Define the output directory\n    output_dir.mkdir(parents=True, exist_ok=True)  # Create the directory if it doesn't exist\n\n    # Important: For operating with page images, we must keep them, otherwise the DocumentConverter\n    # will destroy them for cleaning up memory.\n    # This is done by setting PdfPipelineOptions.images_scale, which also defines the scale of images.\n    # scale=1 correspond of a standard 72 DPI image\n    # The PdfPipelineOptions.generate_* are the selectors for the document elements which will be enriched\n    # with the image field\n    pipeline_options = PdfPipelineOptions()\n    pipeline_options.images_scale = IMAGE_RESOLUTION_SCALE\n    pipeline_options.generate_page_images = True\n    pipeline_options.generate_picture_images = True\n\n    doc_converter = DocumentConverter(\n        format_options={\n            InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n        }\n    )\n\n    start_time = time.time()\n\n    conv_res = doc_converter.convert(input_doc_path)\n\n    output_dir.mkdir(parents=True, exist_ok=True)\n    doc_filename = conv_res.input.file.stem\n\n    # Save page images\n    for page_no, page in conv_res.document.pages.items():\n        page_no = page.page_no\n        page_image_filename = output_dir / f\"{doc_filename}-{page_no}.png\"\n        with page_image_filename.open(\"wb\") as fp:\n            page.image.pil_image.save(fp, format=\"PNG\")\n\n    # Save images of figures and tables\n    table_counter = 0\n    picture_counter = 0\n    for element, _level in conv_res.document.iterate_items():\n        if isinstance(element, TableItem):\n            table_counter += 1\n            element_image_filename = (\n                output_dir / f\"{doc_filename}-table-{table_counter}.png\"\n            )\n            with element_image_filename.open(\"wb\") as fp:\n                element.get_image(conv_res.document).save(fp, \"PNG\")\n\n        if isinstance(element, PictureItem):\n            picture_counter += 1\n            element_image_filename = (\n                output_dir / f\"{doc_filename}-picture-{picture_counter}.png\"\n            )\n            with element_image_filename.open(\"wb\") as fp:\n                element.get_image(conv_res.document).save(fp, \"PNG\")\n\n    # Save markdown with embedded pictures\n    md_filename = output_dir / f\"{doc_filename}-with-images.md\"\n    conv_res.document.save_as_markdown(md_filename, image_mode=ImageRefMode.EMBEDDED)\n\n    # Save markdown with externally referenced pictures\n    md_filename = output_dir / f\"{doc_filename}-with-image-refs.md\"\n    conv_res.document.save_as_markdown(md_filename, image_mode=ImageRefMode.REFERENCED)\n\n    # Save HTML with externally referenced pictures\n    html_filename = output_dir / f\"{doc_filename}-with-image-refs.html\"\n    conv_res.document.save_as_html(html_filename, image_mode=ImageRefMode.REFERENCED)\n\n    end_time = time.time() - start_time\n\n    _log.info(f\"Document converted and figures exported in {end_time:.2f} seconds.\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T09:04:45.778313Z","iopub.execute_input":"2025-01-10T09:04:45.778775Z","iopub.status.idle":"2025-01-10T09:04:45.989543Z","shell.execute_reply.started":"2025-01-10T09:04:45.778746Z","shell.execute_reply":"2025-01-10T09:04:45.988389Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Figure enrichment\n\nLoads a PDF document.\nIdentifies picture elements within the document.\nApplies a simple \"classifier\" (in this case, just adding a dummy classification) to each picture.\nStores the classification results as annotations associated with the picture","metadata":{}},{"cell_type":"code","source":"import logging\nfrom pathlib import Path\nfrom typing import Any, Iterable\nfrom docling_core.types.doc import (\n    DoclingDocument,\n    NodeItem,\n    PictureClassificationClass,\n    PictureClassificationData,\n    PictureItem,\n)\n\nfrom docling.datamodel.base_models import InputFormat\nfrom docling.datamodel.pipeline_options import PdfPipelineOptions\nfrom docling.document_converter import DocumentConverter, PdfFormatOption\nfrom docling.models.base_model import BaseEnrichmentModel\nfrom docling.pipeline.standard_pdf_pipeline import StandardPdfPipeline\n\nclass ExamplePictureClassifierPipelineOptions(PdfPipelineOptions):\n    do_picture_classifer: bool = True\n\nclass ExamplePictureClassifierEnrichmentModel(BaseEnrichmentModel):\n\n    def __init__(self, enabled: bool):\n        self.enabled = enabled\n\n    def is_processable(self, doc: DoclingDocument, element: NodeItem) -> bool:\n        return self.enabled and isinstance(element, PictureItem)\n\n    def __call__(\n        self, doc: DoclingDocument, element_batch: Iterable[NodeItem]\n    ) -> Iterable[Any]:\n        if not self.enabled:\n            return\n\n        for element in element_batch:\n            assert isinstance(element, PictureItem)\n\n            # uncomment this to interactively visualize the image\n            # element.get_image(doc).show()\n\n            element.annotations.append(\n                PictureClassificationData(\n                    provenance=\"example_classifier-0.0.1\",\n                    predicted_classes=[\n                        PictureClassificationClass(class_name=\"dummy\", confidence=0.42)\n                    ],\n                )\n            )\n\n            yield element\n\nclass ExamplePictureClassifierPipeline(StandardPdfPipeline):\n\n    def __init__(self, pipeline_options: ExamplePictureClassifierPipelineOptions):\n        super().__init__(pipeline_options)\n        self.pipeline_options: ExamplePictureClassifierPipeline\n\n        self.enrichment_pipe = [\n            ExamplePictureClassifierEnrichmentModel(\n                enabled=pipeline_options.do_picture_classifer\n            )\n        ]\n\n    @classmethod\n    def get_default_options(cls) -> ExamplePictureClassifierPipelineOptions:\n        return ExamplePictureClassifierPipelineOptions()\n\ndef main():\n    logging.basicConfig(level=logging.INFO)\n\n    input_doc_path = Path(\"/kaggle/input/india-docling-dataset/Himachal Feb-Mar 2024.pdf\")\n\n    pipeline_options = ExamplePictureClassifierPipelineOptions()\n    pipeline_options.images_scale = 2.0\n    pipeline_options.generate_picture_images = True\n\n    doc_converter = DocumentConverter(\n        format_options={\n            InputFormat.PDF: PdfFormatOption(\n                pipeline_cls=ExamplePictureClassifierPipeline,\n                pipeline_options=pipeline_options,\n            )\n        }\n    )\n    result = doc_converter.convert(input_doc_path)\n\n    for element, _level in result.document.iterate_items():\n        if isinstance(element, PictureItem):\n            print(\n                f\"The model populated the `data` portion of picture {element.self_ref}:\\n{element.annotations}\"\n            )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:31:49.098643Z","iopub.execute_input":"2025-01-10T08:31:49.098959Z","iopub.status.idle":"2025-01-10T08:31:49.116681Z","shell.execute_reply.started":"2025-01-10T08:31:49.098937Z","shell.execute_reply":"2025-01-10T08:31:49.115753Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Output of a pdf file Using Figure enrichment","metadata":{}},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:31:51.377379Z","iopub.execute_input":"2025-01-10T08:31:51.377689Z","iopub.status.idle":"2025-01-10T08:31:58.115143Z","shell.execute_reply.started":"2025-01-10T08:31:51.377664Z","shell.execute_reply":"2025-01-10T08:31:58.114366Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Output of .pptx file using Figure enrichment","metadata":{}},{"cell_type":"code","source":"\n\ndef main():\n    logging.basicConfig(level=logging.INFO)\n\n    input_doc_path = Path(\"/kaggle/input/india-docling-dataset/Himachal Feb-Mar 2024.pptx\")\n\n    pipeline_options = ExamplePictureClassifierPipelineOptions()\n    pipeline_options.images_scale = 2.0\n    pipeline_options.generate_picture_images = True\n\n    doc_converter = DocumentConverter(\n        format_options={\n            InputFormat.PDF: PdfFormatOption(\n                pipeline_cls=ExamplePictureClassifierPipeline,\n                pipeline_options=pipeline_options,\n            )\n        }\n    )\n    result = doc_converter.convert(input_doc_path)\n\n    for element, _level in result.document.iterate_items():\n        if isinstance(element, PictureItem):\n            print(\n                f\"The model populated the `data` portion of picture {element.self_ref}:\\n{element.annotations}\"\n            )\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:46:46.813132Z","iopub.execute_input":"2025-01-10T08:46:46.813457Z","iopub.status.idle":"2025-01-10T08:46:47.005576Z","shell.execute_reply.started":"2025-01-10T08:46:46.813434Z","shell.execute_reply":"2025-01-10T08:46:47.004857Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  Output of .docx file using Figure enrichment","metadata":{}},{"cell_type":"code","source":"\ndef main():\n    logging.basicConfig(level=logging.INFO)\n\n    input_doc_path = Path(\"/kaggle/input/india-docling-dataset/Ladakh - June 2024 Ms Anjali.docx\")\n\n    pipeline_options = ExamplePictureClassifierPipelineOptions()\n    pipeline_options.images_scale = 2.0\n    pipeline_options.generate_picture_images = True\n\n    doc_converter = DocumentConverter(\n        format_options={\n            InputFormat.PDF: PdfFormatOption(\n                pipeline_cls=ExamplePictureClassifierPipeline,\n                pipeline_options=pipeline_options,\n            )\n        }\n    )\n    result = doc_converter.convert(input_doc_path)\n\n    for element, _level in result.document.iterate_items():\n        if isinstance(element, PictureItem):\n            print(\n                f\"The model populated the `data` portion of picture {element.self_ref}:\\n{element.annotations}\"\n            )\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T09:10:48.5976Z","iopub.execute_input":"2025-01-10T09:10:48.59788Z","iopub.status.idle":"2025-01-10T09:10:49.293101Z","shell.execute_reply.started":"2025-01-10T09:10:48.597858Z","shell.execute_reply":"2025-01-10T09:10:49.292289Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Conclustion of Figure enrichment this method only support pdf file.\n\nThe figure enrichment method you've provided is currently limited to PDF files due to its reliance on PDF-specific classes, configurations, and internal logic. To process other file types, you would need to create new pipelines and configurations tailored to those specific formats.\nIf you need to process other file types, you'll need to explore the docling library's documentation to see if it provides support for those formats or if you need to build custom pipelines.\n","metadata":{}},{"cell_type":"markdown","source":"# Table export","metadata":{}},{"cell_type":"markdown","source":"## output of a .pptx file of Table export","metadata":{}},{"cell_type":"code","source":"import logging\nimport time\nfrom pathlib import Path\nimport pandas as pd\n\nimport logging\nimport time\nfrom pathlib import Path\n\nfrom docling.document_converter import DocumentConverter\n\n_log = logging.getLogger(__name__)\n\ndef main():\n    logging.basicConfig(level=logging.INFO)\n\n    input_doc_path = Path(\"/kaggle/input/india-docling-dataset/Himachal Feb-Mar 2024.pptx\")\n    output_dir = Path(\"scratch_table_export\")\n\n    doc_converter = DocumentConverter()\n\n    start_time = time.time()\n\n    conv_res = doc_converter.convert(input_doc_path)\n\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    doc_filename = conv_res.input.file.stem\n\n    # Export tables\n    for table_ix, table in enumerate(conv_res.document.tables):\n        table_df: pd.DataFrame = table.export_to_dataframe()\n        print(f\"## Table {table_ix}\")\n        print(table_df.to_markdown())\n\n        # Save the table as csv\n        element_csv_filename = output_dir / f\"{doc_filename}-table-{table_ix+1}.csv\"\n        _log.info(f\"Saving CSV table to {element_csv_filename}\")\n        table_df.to_csv(element_csv_filename, encoding='utf-8') # Specify UTF-8 for CSV\n\n        # Save the table as html\n        element_html_filename = output_dir / f\"{doc_filename}-table-{table_ix+1}.html\"\n        _log.info(f\"Saving HTML table to {element_html_filename}\")\n        with element_html_filename.open(\"w\", encoding='utf-8') as fp: # Specify UTF-8 for HTML\n            fp.write(table.export_to_html())\n\n    end_time = time.time() - start_time\n\n    _log.info(f\"Document converted and tables exported in {end_time:.2f} seconds.\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:52:06.652617Z","iopub.execute_input":"2025-01-10T08:52:06.65292Z","iopub.status.idle":"2025-01-10T08:52:06.845431Z","shell.execute_reply.started":"2025-01-10T08:52:06.652895Z","shell.execute_reply":"2025-01-10T08:52:06.844584Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Output of a .pdf file using Table Export","metadata":{}},{"cell_type":"code","source":"import logging\nimport time\nfrom pathlib import Path\nimport pandas as pd\n\nimport logging\nimport time\nfrom pathlib import Path\n\nfrom docling.document_converter import DocumentConverter\n\n_log = logging.getLogger(__name__)\n\ndef main():\n    logging.basicConfig(level=logging.INFO)\n\n    input_doc_path = Path(\"/kaggle/input/india-docling-dataset/Himachal Feb-Mar 2024.pdf\")\n    output_dir = Path(\"scratch_table_export\")\n\n    doc_converter = DocumentConverter()\n\n    start_time = time.time()\n\n    conv_res = doc_converter.convert(input_doc_path)\n\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    doc_filename = conv_res.input.file.stem\n\n    # Export tables\n    for table_ix, table in enumerate(conv_res.document.tables):\n        table_df: pd.DataFrame = table.export_to_dataframe()\n        print(f\"## Table {table_ix}\")\n        print(table_df.to_markdown())\n\n        # Save the table as csv\n        element_csv_filename = output_dir / f\"{doc_filename}-table-{table_ix+1}.csv\"\n        _log.info(f\"Saving CSV table to {element_csv_filename}\")\n        table_df.to_csv(element_csv_filename, encoding='utf-8') # Specify UTF-8 for CSV\n\n        # Save the table as html\n        element_html_filename = output_dir / f\"{doc_filename}-table-{table_ix+1}.html\"\n        _log.info(f\"Saving HTML table to {element_html_filename}\")\n        with element_html_filename.open(\"w\", encoding='utf-8') as fp: # Specify UTF-8 for HTML\n            fp.write(table.export_to_html())\n\n    end_time = time.time() - start_time\n\n    _log.info(f\"Document converted and tables exported in {end_time:.2f} seconds.\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:54:50.615645Z","iopub.execute_input":"2025-01-10T08:54:50.615969Z","iopub.status.idle":"2025-01-10T08:54:56.921817Z","shell.execute_reply.started":"2025-01-10T08:54:50.615944Z","shell.execute_reply":"2025-01-10T08:54:56.920865Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  Output of a .docx file using Table Export\n\nThis .docx file does not have any table","metadata":{}},{"cell_type":"code","source":"import logging\nimport time\nfrom pathlib import Path\nimport pandas as pd\n\nimport logging\nimport time\nfrom pathlib import Path\n\nfrom docling.document_converter import DocumentConverter\n\n_log = logging.getLogger(__name__)\n\ndef main():\n    logging.basicConfig(level=logging.INFO)\n\n    input_doc_path = Path(\"/kaggle/input/india-docling-dataset/Ladakh.docx\")\n    output_dir = Path(\"scratch_table_export\")\n\n    doc_converter = DocumentConverter()\n\n    start_time = time.time()\n\n    conv_res = doc_converter.convert(input_doc_path)\n\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    doc_filename = conv_res.input.file.stem\n\n    # Export tables\n    for table_ix, table in enumerate(conv_res.document.tables):\n        table_df: pd.DataFrame = table.export_to_dataframe()\n        print(f\"## Table {table_ix}\")\n        print(table_df.to_markdown())\n\n        # Save the table as csv\n        element_csv_filename = output_dir / f\"{doc_filename}-table-{table_ix+1}.csv\"\n        _log.info(f\"Saving CSV table to {element_csv_filename}\")\n        table_df.to_csv(element_csv_filename, encoding='utf-8') # Specify UTF-8 for CSV\n\n        # Save the table as html\n        element_html_filename = output_dir / f\"{doc_filename}-table-{table_ix+1}.html\"\n        _log.info(f\"Saving HTML table to {element_html_filename}\")\n        with element_html_filename.open(\"w\", encoding='utf-8') as fp: # Specify UTF-8 for HTML\n            fp.write(table.export_to_html())\n\n    end_time = time.time() - start_time\n\n    _log.info(f\"Document converted and tables exported in {end_time:.2f} seconds.\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T08:56:06.535756Z","iopub.execute_input":"2025-01-10T08:56:06.536071Z","iopub.status.idle":"2025-01-10T08:56:07.221118Z","shell.execute_reply.started":"2025-01-10T08:56:06.536048Z","shell.execute_reply":"2025-01-10T08:56:07.220477Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Multimodal export\n\nCore Idea: Multimodal Representation of Document Pages\n\nThe code you've provided implements a method for extracting and representing document pages in a multimodal way. This means it captures not just the text content but also the visual layout and structure of each page. This is particularly useful for tasks that require understanding both the text and the visual context of a document, such as:","metadata":{}},{"cell_type":"markdown","source":"## output of a .pdf file using Multimodal export\n","metadata":{}},{"cell_type":"code","source":"import datetime\nimport logging\nimport time\nfrom pathlib import Path\n\nimport pandas as pd\nfrom datasets import Dataset\nfrom PIL import Image\nfrom docling.datamodel.base_models import InputFormat\nfrom docling.datamodel.pipeline_options import PdfPipelineOptions\nfrom docling.document_converter import DocumentConverter, PdfFormatOption\nfrom docling.utils.export import generate_multimodal_pages\nfrom docling.utils.utils import create_hash\n\n\n_log = logging.getLogger(__name__)\n\nIMAGE_RESOLUTION_SCALE = 2.0\n\ndef main():\n    logging.basicConfig(level=logging.INFO)\n\n    input_doc_path = Path(\"/kaggle/input/india-docling-dataset/Himachal Feb-Mar 2024.pdf\")\n    output_dir = Path(\"scratch_multimodal_export\")\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n\n    # Important: For operating with page images, we must keep them, otherwise the DocumentConverter\n    # will destroy them for cleaning up memory.\n    # This is done by setting AssembleOptions.images_scale, which also defines the scale of images.\n    # scale=1 correspond of a standard 72 DPI image\n    pipeline_options = PdfPipelineOptions()\n    pipeline_options.images_scale = IMAGE_RESOLUTION_SCALE\n    pipeline_options.generate_page_images = True\n\n    doc_converter = DocumentConverter(\n        format_options={\n            InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n        }\n    )\n\n    start_time = time.time()\n\n    conv_res = doc_converter.convert(input_doc_path)\n\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    rows = []\n    for (\n        content_text,\n        content_md,\n        content_dt,\n        page_cells,\n        page_segments,\n        page,\n    ) in generate_multimodal_pages(conv_res):\n\n        dpi = page._default_image_scale * 72\n\n        rows.append(\n            {\n                \"document\": conv_res.input.file.name,\n                \"hash\": conv_res.input.document_hash,\n                \"page_hash\": create_hash(\n                    conv_res.input.document_hash + \":\" + str(page.page_no - 1)\n                ),\n                \"image\": {\n                    \"width\": page.image.width,\n                    \"height\": page.image.height,\n                    \"bytes\": page.image.tobytes(),\n                },\n                \"cells\": page_cells,\n                \"contents\": content_text,\n                \"contents_md\": content_md,\n                \"contents_dt\": content_dt,\n                \"segments\": page_segments,\n                \"extra\": {\n                    \"page_num\": page.page_no + 1,\n                    \"width_in_points\": page.size.width,\n                    \"height_in_points\": page.size.height,\n                    \"dpi\": dpi,\n                },\n            }\n        )\n\n    # Generate one parquet from all documents\n    df = pd.json_normalize(rows)\n    now = datetime.datetime.now()\n    output_filename = output_dir / f\"multimodal_{now:%Y-%m-%d_%H%M%S}.parquet\"\n    df.to_parquet(output_filename)\n\n    end_time = time.time() - start_time\n\n    _log.info(\n        f\"Document converted and multimodal pages generated in {end_time:.2f} seconds.\"\n    )\n\n   \n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T09:21:23.897916Z","iopub.execute_input":"2025-01-10T09:21:23.89825Z","iopub.status.idle":"2025-01-10T09:21:32.697509Z","shell.execute_reply.started":"2025-01-10T09:21:23.898217Z","shell.execute_reply":"2025-01-10T09:21:32.696764Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv(\"/kaggle/working/scratch_table_export/Himachal Feb-Mar 2024-table-1.csv\")\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T09:24:27.87251Z","iopub.execute_input":"2025-01-10T09:24:27.872865Z","iopub.status.idle":"2025-01-10T09:24:27.888067Z","shell.execute_reply.started":"2025-01-10T09:24:27.872836Z","shell.execute_reply":"2025-01-10T09:24:27.887216Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This is not usefull","metadata":{}},{"cell_type":"markdown","source":"## Also can upload this csv file in hugging face using this code\nThis block demonstrates how the file can be opened with the HF datasets library\n    # from datasets import Dataset\n    # from PIL import Image\n    # multimodal_df = pd.read_parquet(output_filename)\n\n    # # Convert pandas DataFrame to Hugging Face Dataset and load bytes into image\n    # dataset = Dataset.from_pandas(multimodal_df)\n    # def transforms(examples):\n    #     examples[\"image\"] = Image.frombytes('RGB', (examples[\"image.width\"], examples[\"image.height\"]), examples[\"image.bytes\"], 'raw')\n    #     return examples\n    # dataset = dataset.map(transforms)","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## output of a .pptx file using Multimodal export\n\nYou've hit a crucial point! The IndexError: list index out of range error, specifically within the docling.utils.export.generate_multimodal_pages function, strongly suggests that the multimodal export method, as it's currently implemented, does not fully support PPTX files.","metadata":{}},{"cell_type":"code","source":"import datetime\nimport logging\nimport time\nfrom pathlib import Path\n\nimport pandas as pd\nfrom datasets import Dataset\nfrom PIL import Image\nfrom docling.datamodel.base_models import InputFormat\nfrom docling.datamodel.pipeline_options import PdfPipelineOptions\nfrom docling.document_converter import DocumentConverter, PdfFormatOption\nfrom docling.utils.export import generate_multimodal_pages\nfrom docling.utils.utils import create_hash\n\n\n_log = logging.getLogger(__name__)\n\nIMAGE_RESOLUTION_SCALE = 2.0\n\ndef main():\n    logging.basicConfig(level=logging.INFO)\n\n    input_doc_path = Path(\"/kaggle/input/india-docling-dataset/Himachal Feb-Mar 2024.pptx\")\n    output_dir = Path(\"scratch_multimodal_export\")\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n\n    # Important: For operating with page images, we must keep them, otherwise the DocumentConverter\n    # will destroy them for cleaning up memory.\n    # This is done by setting AssembleOptions.images_scale, which also defines the scale of images.\n    # scale=1 correspond of a standard 72 DPI image\n    pipeline_options = PdfPipelineOptions()\n    pipeline_options.images_scale = IMAGE_RESOLUTION_SCALE\n    pipeline_options.generate_page_images = True\n\n    doc_converter = DocumentConverter(\n        format_options={\n            InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n        }\n    )\n\n    start_time = time.time()\n\n    conv_res = doc_converter.convert(input_doc_path)\n\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    rows = []\n    for (\n        content_text,\n        content_md,\n        content_dt,\n        page_cells,\n        page_segments,\n        page,\n    ) in generate_multimodal_pages(conv_res):\n\n        dpi = page._default_image_scale * 72\n\n        rows.append(\n            {\n                \"document\": conv_res.input.file.name,\n                \"hash\": conv_res.input.document_hash,\n                \"page_hash\": create_hash(\n                    conv_res.input.document_hash + \":\" + str(page.page_no - 1)\n                ),\n                \"image\": {\n                    \"width\": page.image.width,\n                    \"height\": page.image.height,\n                    \"bytes\": page.image.tobytes(),\n                },\n                \"cells\": page_cells,\n                \"contents\": content_text,\n                \"contents_md\": content_md,\n                \"contents_dt\": content_dt,\n                \"segments\": page_segments,\n                \"extra\": {\n                    \"page_num\": page.page_no + 1,\n                    \"width_in_points\": page.size.width,\n                    \"height_in_points\": page.size.height,\n                    \"dpi\": dpi,\n                },\n            }\n        )\n\n    # Generate one parquet from all documents\n    df = pd.json_normalize(rows)\n    now = datetime.datetime.now()\n    output_filename = output_dir / f\"multimodal_{now:%Y-%m-%d_%H%M%S}.parquet\"\n    df.to_parquet(output_filename)\n\n    end_time = time.time() - start_time\n\n    _log.info(\n        f\"Document converted and multimodal pages generated in {end_time:.2f} seconds.\"\n    )\n\n   \n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T09:26:18.078133Z","iopub.execute_input":"2025-01-10T09:26:18.078505Z","iopub.status.idle":"2025-01-10T09:26:18.309415Z","shell.execute_reply.started":"2025-01-10T09:26:18.078479Z","shell.execute_reply":"2025-01-10T09:26:18.308245Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## output of a .docx file using Multimodal export","metadata":{}},{"cell_type":"code","source":"import datetime\nimport logging\nimport time\nfrom pathlib import Path\n\nimport pandas as pd\nfrom datasets import Dataset\nfrom PIL import Image\nfrom docling.datamodel.base_models import InputFormat\nfrom docling.datamodel.pipeline_options import PdfPipelineOptions\nfrom docling.document_converter import DocumentConverter, PdfFormatOption\nfrom docling.utils.export import generate_multimodal_pages\nfrom docling.utils.utils import create_hash\n\n\n_log = logging.getLogger(__name__)\n\nIMAGE_RESOLUTION_SCALE = 2.0\n\ndef main():\n    logging.basicConfig(level=logging.INFO)\n\n    input_doc_path = Path(\"/kaggle/input/india-docling-dataset/Ladakh.docx\")\n    output_dir = Path(\"scratch_multimodal_export\")\n    #output_dir.mkdir(parents=True, exist_ok=True)\n\n\n    # Important: For operating with page images, we must keep them, otherwise the DocumentConverter\n    # will destroy them for cleaning up memory.\n    # This is done by setting AssembleOptions.images_scale, which also defines the scale of images.\n    # scale=1 correspond of a standard 72 DPI image\n    pipeline_options = PdfPipelineOptions()\n    pipeline_options.images_scale = IMAGE_RESOLUTION_SCALE\n    pipeline_options.generate_page_images = True\n\n    doc_converter = DocumentConverter(\n        format_options={\n            InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n        }\n    )\n\n    start_time = time.time()\n\n    conv_res = doc_converter.convert(input_doc_path)\n\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    rows = []\n    for (\n        content_text,\n        content_md,\n        content_dt,\n        page_cells,\n        page_segments,\n        page,\n    ) in generate_multimodal_pages(conv_res):\n\n        dpi = page._default_image_scale * 72\n\n        rows.append(\n            {\n                \"document\": conv_res.input.file.name,\n                \"hash\": conv_res.input.document_hash,\n                \"page_hash\": create_hash(\n                    conv_res.input.document_hash + \":\" + str(page.page_no - 1)\n                ),\n                \"image\": {\n                    \"width\": page.image.width,\n                    \"height\": page.image.height,\n                    \"bytes\": page.image.tobytes(),\n                },\n                \"cells\": page_cells,\n                \"contents\": content_text,\n                \"contents_md\": content_md,\n                \"contents_dt\": content_dt,\n                \"segments\": page_segments,\n                \"extra\": {\n                    \"page_num\": page.page_no + 1,\n                    \"width_in_points\": page.size.width,\n                    \"height_in_points\": page.size.height,\n                    \"dpi\": dpi,\n                },\n            }\n        )\n\n    # Generate one parquet from all documents\n    df = pd.json_normalize(rows)\n    now = datetime.datetime.now()\n    output_filename = output_dir / f\"multimodal_{now:%Y-%m-%d_%H%M%S}.parquet\"\n    df.to_parquet(output_filename)\n\n    end_time = time.time() - start_time\n\n    _log.info(\n        f\"Document converted and multimodal pages generated in {end_time:.2f} seconds.\"\n    )\n\n   \n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T09:27:36.678265Z","iopub.execute_input":"2025-01-10T09:27:36.67858Z","iopub.status.idle":"2025-01-10T09:27:37.366457Z","shell.execute_reply.started":"2025-01-10T09:27:36.678557Z","shell.execute_reply":"2025-01-10T09:27:37.365572Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Define the path to your Parquet file\nparquet_file_path = \"/kaggle/working/scratch_multimodal_export/multimodal_2025-01-10_092132.parquet\"\n\ntry:\n    # Read the Parquet file into a pandas DataFrame\n    multimodal_df = pd.read_parquet(parquet_file_path)\n\n    # Display the DataFrame\n    print(\"DataFrame head:\")\n    print(multimodal_df.head())  # Display the first few rows\n\n    print(\"\\nDataFrame info:\")\n    print(multimodal_df.info())  # Display DataFrame info\n\n    # Display a sample of the data\n    print(\"\\nSample of the data:\")\n    print(multimodal_df.sample(5)) # Display 5 random rows\n\nexcept FileNotFoundError:\n    print(f\"Error: Parquet file not found at {parquet_file_path}\")\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T09:30:21.96417Z","iopub.execute_input":"2025-01-10T09:30:21.96455Z","iopub.status.idle":"2025-01-10T09:30:22.860775Z","shell.execute_reply.started":"2025-01-10T09:30:21.964513Z","shell.execute_reply":"2025-01-10T09:30:22.859938Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"multimodal_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T09:31:02.603499Z","iopub.execute_input":"2025-01-10T09:31:02.603833Z","iopub.status.idle":"2025-01-10T09:31:03.41331Z","shell.execute_reply.started":"2025-01-10T09:31:02.603804Z","shell.execute_reply":"2025-01-10T09:31:03.412406Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Force full page OCR\nCore Idea: Performing OCR on a PDF Document\n\nThe code you've provided demonstrates how to use the docling library to perform OCR on a PDF document. It specifically focuses on the \"force full page OCR\" option, which dictates how the OCR process is applied to the document's pages.\n\n\nHow It Works\n\nThe code works by:\n\nLoading a PDF document.\n\nConfiguring the PDF processing pipeline to enable OCR, table structure detection, and cell matching.\n\nSelecting an OCR engine and setting the force_full_page_ocr option to True.\n\nConverting the PDF document using the specified pipeline options.\n\nDuring the conversion process, the OCR engine is applied to each page.\n\nThe OCR engine processes the entire page due to the force_full_page_ocr setting.\n\nThe converted document is exported to Markdown format.\n\nThe Markdown output is printed.\n\n","metadata":{}},{"cell_type":"markdown","source":"## output of .pdf file using Force full page OCR","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\nfrom docling.backend.docling_parse_backend import DoclingParseDocumentBackend\nfrom docling.datamodel.base_models import InputFormat\nfrom docling.datamodel.pipeline_options import (\n    EasyOcrOptions,\n    OcrMacOptions,\n    PdfPipelineOptions,\n    RapidOcrOptions,\n    TesseractCliOcrOptions,\n    TesseractOcrOptions,\n)\nfrom docling.document_converter import DocumentConverter, PdfFormatOption\n\n\ndef main():\n    input_doc = Path(\"/kaggle/input/india-docling-dataset/Himachal Feb-Mar 2024.pdf\")\n\n    pipeline_options = PdfPipelineOptions()\n    pipeline_options.do_ocr = True\n    pipeline_options.do_table_structure = True\n    pipeline_options.table_structure_options.do_cell_matching = True\n\n    # Any of the OCR options can be used:EasyOcrOptions, TesseractOcrOptions, TesseractCliOcrOptions, OcrMacOptions(Mac only), RapidOcrOptions\n    # ocr_options = EasyOcrOptions(force_full_page_ocr=True)\n    # ocr_options = TesseractOcrOptions(force_full_page_ocr=True)\n    # ocr_options = OcrMacOptions(force_full_page_ocr=True)\n    # ocr_options = RapidOcrOptions(force_full_page_ocr=True)\n    ocr_options = TesseractCliOcrOptions(force_full_page_ocr=True)\n    pipeline_options.ocr_options = ocr_options\n\n    converter = DocumentConverter(\n        format_options={\n            InputFormat.PDF: PdfFormatOption(\n                pipeline_options=pipeline_options,\n            )\n        }\n    )\n\n    doc = converter.convert(input_doc).document\n    md = doc.export_to_markdown()\n    print(md)\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T09:38:28.342844Z","iopub.execute_input":"2025-01-10T09:38:28.34324Z","iopub.status.idle":"2025-01-10T09:38:40.623715Z","shell.execute_reply.started":"2025-01-10T09:38:28.343173Z","shell.execute_reply":"2025-01-10T09:38:40.622755Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## output of .pptx file using Force full page OCR","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\nfrom docling.backend.docling_parse_backend import DoclingParseDocumentBackend\nfrom docling.datamodel.base_models import InputFormat\nfrom docling.datamodel.pipeline_options import (\n    EasyOcrOptions,\n    OcrMacOptions,\n    PdfPipelineOptions,\n    RapidOcrOptions,\n    TesseractCliOcrOptions,\n    TesseractOcrOptions,\n)\nfrom docling.document_converter import DocumentConverter, PdfFormatOption\n\n\ndef main():\n    input_doc = Path(\"/kaggle/input/india-docling-dataset/Himachal Feb-Mar 2024.pptx\")\n\n    pipeline_options = PdfPipelineOptions()\n    pipeline_options.do_ocr = True\n    pipeline_options.do_table_structure = True\n    pipeline_options.table_structure_options.do_cell_matching = True\n\n    # Any of the OCR options can be used:EasyOcrOptions, TesseractOcrOptions, TesseractCliOcrOptions, OcrMacOptions(Mac only), RapidOcrOptions\n    # ocr_options = EasyOcrOptions(force_full_page_ocr=True)\n    # ocr_options = TesseractOcrOptions(force_full_page_ocr=True)\n    # ocr_options = OcrMacOptions(force_full_page_ocr=True)\n    # ocr_options = RapidOcrOptions(force_full_page_ocr=True)\n    ocr_options = TesseractCliOcrOptions(force_full_page_ocr=True)\n    pipeline_options.ocr_options = ocr_options\n\n    converter = DocumentConverter(\n        format_options={\n            InputFormat.PDF: PdfFormatOption(\n                pipeline_options=pipeline_options,\n            )\n        }\n    )\n\n    doc = converter.convert(input_doc).document\n    md = doc.export_to_markdown()\n    print(md)\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T09:40:02.437947Z","iopub.execute_input":"2025-01-10T09:40:02.438283Z","iopub.status.idle":"2025-01-10T09:40:02.627978Z","shell.execute_reply.started":"2025-01-10T09:40:02.438255Z","shell.execute_reply":"2025-01-10T09:40:02.627161Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## output of .pptx file using Force full page OCR","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\nfrom docling.backend.docling_parse_backend import DoclingParseDocumentBackend\nfrom docling.datamodel.base_models import InputFormat\nfrom docling.datamodel.pipeline_options import (\n    EasyOcrOptions,\n    OcrMacOptions,\n    PdfPipelineOptions,\n    RapidOcrOptions,\n    TesseractCliOcrOptions,\n    TesseractOcrOptions,\n)\nfrom docling.document_converter import DocumentConverter, PdfFormatOption\n\n\ndef main():\n    input_doc = Path(\"/kaggle/input/india-docling-dataset/Ladakh.docx\")\n\n    pipeline_options = PdfPipelineOptions()\n    pipeline_options.do_ocr = True\n    pipeline_options.do_table_structure = True\n    pipeline_options.table_structure_options.do_cell_matching = True\n\n    # Any of the OCR options can be used:EasyOcrOptions, TesseractOcrOptions, TesseractCliOcrOptions, OcrMacOptions(Mac only), RapidOcrOptions\n    # ocr_options = EasyOcrOptions(force_full_page_ocr=True)\n    # ocr_options = TesseractOcrOptions(force_full_page_ocr=True)\n    # ocr_options = OcrMacOptions(force_full_page_ocr=True)\n    # ocr_options = RapidOcrOptions(force_full_page_ocr=True)\n    ocr_options = TesseractCliOcrOptions(force_full_page_ocr=True)\n    pipeline_options.ocr_options = ocr_options\n\n    converter = DocumentConverter(\n        format_options={\n            InputFormat.PDF: PdfFormatOption(\n                pipeline_options=pipeline_options,\n            )\n        }\n    )\n\n    doc = converter.convert(input_doc).document\n    md = doc.export_to_markdown()\n    print(md)\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T09:40:34.708225Z","iopub.execute_input":"2025-01-10T09:40:34.70858Z","iopub.status.idle":"2025-01-10T09:40:35.378125Z","shell.execute_reply.started":"2025-01-10T09:40:34.708553Z","shell.execute_reply":"2025-01-10T09:40:35.377267Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Accelerator options\nCore Idea: Utilizing Hardware Acceleration for Document Conversion\n\nThe code you've provided demonstrates how to use the docling library to leverage hardware acceleration (CPU, GPU, or MPS) to speed up the document conversion process, particularly the OCR and table structure detection steps.\n\n","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\nfrom docling.backend.docling_parse_backend import DoclingParseDocumentBackend\nfrom docling.datamodel.base_models import InputFormat\nfrom docling.datamodel.pipeline_options import (\n    AcceleratorDevice,\n    AcceleratorOptions,\n    PdfPipelineOptions,\n    TesseractCliOcrOptions,\n    TesseractOcrOptions,\n)\nfrom docling.datamodel.settings import settings\nfrom docling.document_converter import DocumentConverter, PdfFormatOption\n\ndef main():\n    input_doc = Path(\"/kaggle/input/india-docling-dataset/Ladakh - June 2024 Ms Anjali.pdf\")\n\n    # Explicitly set the accelerator\n    # accelerator_options = AcceleratorOptions(\n    #     num_threads=8, device=AcceleratorDevice.AUTO\n    # )\n    accelerator_options = AcceleratorOptions(\n        num_threads=8, device=AcceleratorDevice.CPU\n    )\n    # accelerator_options = AcceleratorOptions(\n    #     num_threads=8, device=AcceleratorDevice.MPS\n    # )\n    # accelerator_options = AcceleratorOptions(\n    #     num_threads=8, device=AcceleratorDevice.CUDA\n    # )\n\n    pipeline_options = PdfPipelineOptions()\n    pipeline_options.accelerator_options = accelerator_options\n    pipeline_options.do_ocr = True\n    pipeline_options.do_table_structure = True\n    pipeline_options.table_structure_options.do_cell_matching = True\n\n    converter = DocumentConverter(\n        format_options={\n            InputFormat.PDF: PdfFormatOption(\n                pipeline_options=pipeline_options,\n            )\n        }\n    )\n\n    # Enable the profiling to measure the time spent\n    settings.debug.profile_pipeline_timings = True\n\n    # Convert the document\n    conversion_result = converter.convert(input_doc)\n    doc = conversion_result.document\n\n    # List with total time per document\n    doc_conversion_secs = conversion_result.timings[\"pipeline_total\"].times\n\n    md = doc.export_to_markdown()\n    print(md)\n    print(f\"Conversion secs: {doc_conversion_secs}\")\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T09:42:51.818315Z","iopub.execute_input":"2025-01-10T09:42:51.818691Z","iopub.status.idle":"2025-01-10T09:43:16.198404Z","shell.execute_reply.started":"2025-01-10T09:42:51.818661Z","shell.execute_reply":"2025-01-10T09:43:16.197633Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}